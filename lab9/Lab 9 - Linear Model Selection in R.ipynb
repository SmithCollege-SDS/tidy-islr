{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Model Validation using Validation and Cross-Validation in R comes from p. 248-251 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "library(leaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using the Validation Set Approach\n",
    "\n",
    "In Lab 8, we saw that it is possible to choose among a set of models of different\n",
    "sizes using $C_p$, BIC, and adjusted $R^2$. We will now consider how to do this\n",
    "using the validation set and cross-validation approaches.\n",
    "\n",
    "As in Lab 8, we'll be working with the ${\\tt Hitters}$ dataset from ${\\tt ISLR}$. Since we're trying to predict ${\\tt Salary}$ and we know from last time that some are missing, let's first drop all the rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hitters=na.omit(Hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for these approaches to yield accurate estimates of the test\n",
    "error, we must use *only the training observations* to perform all aspects of\n",
    "model-fitting â€” including variable selection. Therefore, the determination of\n",
    "which model of a given size is best must be made using *only the training\n",
    "observations*. This point is subtle but important. If the full data set is used\n",
    "to perform the best subset selection step, the validation set errors and\n",
    "cross-validation errors that we obtain will not be accurate estimates of the\n",
    "test error.\n",
    "\n",
    "In order to use the validation set approach, we begin by splitting the\n",
    "observations into a training set and a test set. We do this by creating\n",
    "a random vector, train, of elements equal to TRUE if the corresponding\n",
    "observation is in the training set, and FALSE otherwise. The vector test has\n",
    "a TRUE if the observation is in the test set, and a FALSE otherwise. Note the\n",
    "! in the command to create test causes TRUEs to be switched to FALSEs and\n",
    "vice versa. We also set a random seed so that the user will obtain the same\n",
    "training set/test set split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)\n",
    "test=(!train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply ${\\tt regsubsets()}$ to the training set in order to perform best\n",
    "subset selection\\*.\n",
    "\n",
    "(\\* Note: If you're trying to complete this lab on a machine that can't handle **best subset**, try forward or backward selection instead by adding the ${\\tt method = \"forward\"}$ or ${\\tt method = \"backward\"}$ parameter to your call to ${\\tt regsubsets()}$. You'll get slightly different values, but the concepts are the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit.best.train=regsubsets(Salary~.,data=Hitters [train ,], nvmax = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we subset the ${\\tt Hitters}$ data frame directly in the call in order\n",
    "to access only the training subset of the data, using the expression\n",
    "${\\tt Hitters[train,]}$. We now compute the validation set error for the best\n",
    "model of each model size. We first make a model matrix from the test\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.mat=model.matrix (Salary~.,data=Hitters [test ,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ${\\tt model.matrix()}$ function is used in many regression packages for building an $X$ matrix from data. Now we run a loop, and for each size $i$, we\n",
    "extract the coefficients from ${\\tt regfit.best}$ for the best model of that size,\n",
    "multiply them into the appropriate columns of the test model matrix to\n",
    "form the predictions, and compute the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val.errors=rep(NA,19)\n",
    "\n",
    "# Iterates over each size i\n",
    "for(i in 1:19){\n",
    "    \n",
    "    # Extract the vector of predictors in the best fit model on i predictors\n",
    "    coefi=coef(regfit.best.train,id=i)\n",
    "    \n",
    "    # Make predictions using matrix multiplication of the test matirx and the coefficients vector\n",
    "    pred=test.mat[,names(coefi)]%*%coefi\n",
    "    \n",
    "    # Calculate the MSE\n",
    "    val.errors[i]=mean((Hitters$Salary[test]-pred)^2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the errors, and find the model that minimizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the model with the smallest error\n",
    "min = which.min(val.errors)\n",
    "\n",
    "# Plot the errors for each model size\n",
    "plot(val.errors,type='b')\n",
    "points(min,val.errors[min][1], col =\"red\",cex =2, pch =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! We find that the best model (according to the validation set approach) is the one that contains 10 predictors.\n",
    "\n",
    "This was a little tedious, partly because there is no ${\\tt predict()}$ method\n",
    "for ${\\tt regsubsets()}$. Since we will be using this function again, we can capture\n",
    "our steps above and write our own ${\\tt predict()}$ method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict.regsubsets=function(object,newdata,id,...){\n",
    "      form=as.formula(object$call[[2]]) # Extract the formula used when we called regsubsets()\n",
    "      mat=model.matrix(form,newdata)    # Build the model matrix\n",
    "      coefi=coef(object,id=id)          # Extract the coefficiants of the ith model\n",
    "      xvars=names(coefi)                # Pull out the names of the predictors used in the ith model\n",
    "      mat[,xvars]%*%coefi               # Make predictions using matrix multiplication\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pretty much mimics what we did above. The one tricky\n",
    "part is how we extracted the formula used in the call to ${\\tt regsubsets()}$, but you don't need to worry too much about the mechanisc of this right now. We'll use this function to make our lives a little easier when we do cross-validation.\n",
    "\n",
    "Now that we know what we're looking for, let's perform best subset selection on the full dataset (up to 10 predictors) and select the best 10-predictor model. It is important that we make use of the *full\n",
    "data set* in order to obtain more accurate coefficient estimates. Note that\n",
    "we perform best subset selection on the full data set and select the best 10-predictor\n",
    "model, rather than simply using the predictors that we obtained\n",
    "from the training set, because the best 10-predictor model on the full data\n",
    "set may differ from the corresponding model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit.best=regsubsets(Salary~.,data=Hitters ,nvmax=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we see that the best ten-variable model on the full data set has a\n",
    "**different set of predictors** than the best ten-variable model on the training\n",
    "set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef(regfit.best,10)\n",
    "coef(regfit.best.train,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using Cross-Validation\n",
    "\n",
    "Now let's try to choose among the models of different sizes using cross-validation.\n",
    "This approach is somewhat involved, as we must perform best\n",
    "subset selection\\* within each of the $k$ training sets. Despite this, we see that\n",
    "with its clever subsetting syntax, ${\\tt R}$ makes this job quite easy. First, we\n",
    "create a vector that assigns each observation to one of $k = 10$ folds, and\n",
    "we create a matrix in which we will store the results:\n",
    "\n",
    "\\* or forward selection / backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=10        # number of folds\n",
    "set.seed(1) # set the random seed so we all get the same results\n",
    "\n",
    "# Assign each observation to a single fold\n",
    "folds=sample(1:k,nrow(Hitters ), replace = TRUE)\n",
    "\n",
    "# Create a matrix to store the results of our upcoming calculations\n",
    "cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a for loop that performs cross-validation. In the $j^{th}$ fold, the\n",
    "elements of folds that equal $j$ are in the test set, and the remainder are in\n",
    "the training set. We make our predictions for each model size (using our\n",
    "new $predict()$ method), compute the test errors on the appropriate subset,\n",
    "and store them in the appropriate slot in the matrix ${\\tt cv.errors}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer loop iterates over all folds\n",
    "for(j in 1:k){\n",
    "    \n",
    "    # The perform best subset selection on the full dataset, minus the jth fold\n",
    "    best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)\n",
    "    \n",
    "    # Inner loop iterates over each size i\n",
    "    for(i in 1:19){\n",
    "        \n",
    "        # Predict the values of the current fold from the \"best subset\" model on i predictors\n",
    "        pred=predict(best.fit,Hitters[folds==j,],id=i)\n",
    "        \n",
    "        # Calculate the MSE, store it in the matrix we created above\n",
    "        cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has filled up the ${\\tt cv.errors}$ matrix such that the $(i,j)^{th}$ element corresponds\n",
    "to the test MSE for the $i^{th}$ cross-validation fold for the best $j$-variable\n",
    "model.  We can then use the ${\\tt apply()}$ function to take the ${\\tt mean}$ over the columns of this\n",
    "matrix. This will give us a vector for which the $j^{th}$ element is the cross-validation\n",
    "error for the $j$-variable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the mean of over all folds for each model size\n",
    "mean.cv.errors = apply(cv.errors, 2, mean)\n",
    "\n",
    "# Find the model size with the smallest cross-validation error\n",
    "min = which.min(mean.cv.errors)\n",
    "\n",
    "# Plot the cross-validation error for each model size, highlight the min\n",
    "plot(mean.cv.errors,type='b')\n",
    "points(min,mean.cv.errors[min][1], col =\"red\",cex =2, pch =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that cross-validation selects an 11-predictor model. Now let's use\n",
    "best subset selection on the full data set in order to obtain the 11-predictor\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg.best=regsubsets(Salary~.,data=Hitters , nvmax =19)\n",
    "coef(reg.best,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's also take a look at the statistics from last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "\n",
    "reg.summary = summary(reg.best)\n",
    "\n",
    "# Plot RSS\n",
    "plot(reg.summary$rss,xlab=\"Number of Variables\",ylab=\"RSS\", type=\"l\")\n",
    "\n",
    "# Plot Adjusted R^2, highlight max value\n",
    "plot(reg.summary$adjr2,xlab=\"Number of Variables\", ylab=\" Adjusted RSq\",type=\"l\")\n",
    "max = which.max(reg.summary$adjr2)\n",
    "points(max,reg.summary$adjr2[max], col =\"red\",cex =2, pch =20)\n",
    "\n",
    "# Plot Cp, highlight min value\n",
    "plot(reg.summary$cp,xlab=\"Number of Variables\",ylab=\"Cp\", type=\"l\")\n",
    "min = which.min(reg.summary$cp)\n",
    "points(min,reg.summary$cp[min],col=\"red\",cex=2,pch=20)\n",
    "\n",
    "# Plot BIC, highlight min value\n",
    "plot(reg.summary$bic,xlab=\"Number of Variables\",ylab=\"BIC\", type=\"l\")\n",
    "min = which.min(reg.summary$bic)\n",
    "points(min,reg.summary$bic[min],col=\"red\",cex=2,pch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some of the indicators agree with the cross-validated model, and others are very different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test out these approaches (best / forward / backward selection) and evaluation methods (adjusted training error, validation set, cross validation) on other datasets. You may want to work with a team on this portion of the lab.\n",
    "\n",
    "You may use any of the datasets included in the ${\\tt ISLR}$ package, or choose one from the UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets.html). Download a dataset, and try to determine the optimal set of parameters to use to model it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get credit for this lab, please post your answers to the following questions:\n",
    " - What dataset did you choose?\n",
    " - Which selection techniques did you try?\n",
    " - Which evaluation techniques did you try?\n",
    " - What did you determine was the best set of parameters to model this data?\n",
    " - How well did this model perform?\n",
    " \n",
    "to Piazza: https://piazza.com/class/igwiv4w3ctb6rg?cid=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
